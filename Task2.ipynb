{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Stock Prices Time Series Prediction\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sp500.csv')\n",
    "data = data.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Highest correlation\n",
    "Get the single column with the highest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_correlations = np.array(correlation_matrix[['SP500']])[1:]\n",
    "correlation_index = np.argmax(sp_correlations) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract The Desired Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_stock = data.iloc[:, correlation_index]\n",
    "\n",
    "input_set_1d  = np.array(desired_stock)\n",
    "output_set_1d = np.array(desired_stock.shift(-5).dropna())\n",
    "\n",
    "def construct_time_series(best_stock_id,data):\n",
    "    x = []\n",
    "    y = []\n",
    "    windowSize = 4\n",
    "    for i in range(data.shape[0] - 2 * windowSize):\n",
    "        x += [data[best_stock_id][i:i + windowSize+1]]\n",
    "        y += [data[best_stock_id][i + windowSize + 1:i + 2 *windowSize + 1]]\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "input_set, output_set = construct_time_series(data.columns[correlation_index],data)\n",
    "\n",
    "train_ratio = 0.8\n",
    "test_ratio  = 0.2\n",
    "valid_ratio = 0.1 # Will be used in the MLPClassifier to take 10% of the training data as validation\n",
    "\n",
    "training_input, testing_input, training_target, testing_target = train_test_split(input_set,output_set,\n",
    "                                                                                  test_size=test_ratio,\n",
    "                                                                                  random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Linear regressor\n",
    "Two multilayer perceptrons. One using a stochastic gradient descent optimizer, and the other using an adam optimizer.\n",
    "\n",
    "Other than the optimizer, all other parameters will remain consistent for both models.\n",
    "\n",
    "#### Scoring Metric Used:\n",
    "*The mean suared error will be used as a scoring metric. The lower the error, the better the optimizer!*\n",
    "\n",
    "#### Regularization Technique Used:\n",
    "*L2 regularization was used to avoid overfitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1001, 5)\n",
      "(1001, 4)\n"
     ]
    }
   ],
   "source": [
    "hidden_layer        = (512, 512, 512) # Hidden layers and nodes, per layer\n",
    "regularization_rate = 0.00001         # Rate for L2 regularization\n",
    "learning_rate_start = 0.01            # Initial learning rate\n",
    "learning_rate_mode  = 'adaptive'      # Mode for changing learning rate\n",
    "batch_size          = 128             # Size of batch for one update\n",
    "tolerance           = 1e-6            # Tolerance level for regressor\n",
    "max_iteration       = 1000            # Maximum number of iterations before stopping\n",
    "no_change_tolerance = 10              # Stop if no significant change happens in this number of iterations\n",
    "\n",
    "# n_iter_no_change=10\n",
    "\n",
    "model_adm = MLPRegressor(hidden_layer_sizes=hidden_layer, solver='adam', alpha=regularization_rate,\n",
    "                          batch_size=batch_size, learning_rate=learning_rate_mode,\n",
    "                          learning_rate_init=learning_rate_start, max_iter=max_iteration, tol=tolerance,\n",
    "                          verbose=True, early_stopping=True, validation_fraction=valid_ratio,\n",
    "                          n_iter_no_change=no_change_tolerance)\n",
    "model_sgd = MLPRegressor(hidden_layer_sizes=hidden_layer, solver='sgd', alpha=regularization_rate,\n",
    "                          batch_size=batch_size, learning_rate=learning_rate_mode,\n",
    "                          learning_rate_init=learning_rate_start, max_iter=max_iteration, tol=tolerance,\n",
    "                          verbose=True, early_stopping=True, validation_fraction=valid_ratio,\n",
    "                          n_iter_no_change=no_change_tolerance)\n",
    "\n",
    "print(training_input.shape)\n",
    "print(training_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give The Models The Training Data And Fit Them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, The SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2758.37354507\n",
      "Validation score: -2.688277\n",
      "Iteration 2, loss = 173.52461618\n",
      "Validation score: 0.669108\n",
      "Iteration 3, loss = 27.48855758\n",
      "Validation score: 0.982521\n",
      "Iteration 4, loss = 8.16405745\n",
      "Validation score: 0.957312\n",
      "Iteration 5, loss = 2.38889970\n",
      "Validation score: 0.991974\n",
      "Iteration 6, loss = 0.86143929\n",
      "Validation score: 0.990242\n",
      "Iteration 7, loss = 1.11978942\n",
      "Validation score: 0.989185\n",
      "Iteration 8, loss = 1.75836021\n",
      "Validation score: 0.987719\n",
      "Iteration 9, loss = 3.17086889\n",
      "Validation score: 0.961061\n",
      "Iteration 10, loss = 2.29786606\n",
      "Validation score: 0.969295\n",
      "Iteration 11, loss = 1.85459794\n",
      "Validation score: 0.987515\n",
      "Iteration 12, loss = 1.82513335\n",
      "Validation score: 0.916205\n",
      "Iteration 13, loss = 6.22683849\n",
      "Validation score: 0.932712\n",
      "Iteration 14, loss = 3.60527545\n",
      "Validation score: 0.984215\n",
      "Iteration 15, loss = 1.93428558\n",
      "Validation score: 0.981742\n",
      "Iteration 16, loss = 1.71935371\n",
      "Validation score: 0.991502\n",
      "Validation score did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size=128, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(512, 512, 512), learning_rate='adaptive',\n",
       "       learning_rate_init=0.01, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=1e-06,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adm.fit(training_input, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam's Mean-Squared-Error: 1.3787097563770685\n"
     ]
    }
   ],
   "source": [
    "predictions_adm = model_adm.predict(testing_input)\n",
    "mse_adm = metrics.mean_squared_error(predictions_adm,testing_target)\n",
    "print(\"Adam's Mean-Squared-Error:\", mse_adm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
